{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Import dependancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To mute annoying warnings in notebook\n",
    "import warnings\n",
    "\n",
    "# For Data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Math plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import (\n",
    "    preprocessing,\n",
    "    metrics,\n",
    "    decomposition,\n",
    "    cluster,\n",
    ")\n",
    "\n",
    "from clustergram import Clustergram\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Getting data, observations\n",
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset from file\n",
    "data = pd.read_csv(\n",
    "    \"../data/SouthGermanCredit.asc\",\n",
    "    delimiter=\" \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Processing\n",
    "\n",
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Choose data and cluster amount for agglomerative and k-means methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scaled data\n",
    "data_to_clustering = X_scaled\n",
    "\n",
    "# Get maximal cluster amount\n",
    "cluster_amount = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set metric data lists\n",
    "silhouette_agglomerative = []\n",
    "davies_agglomerative = []\n",
    "calinski_agglomerative = []\n",
    "distortions_agglomerative = []\n",
    "\n",
    "# Iterate over cluster number\n",
    "for i in range(2, cluster_amount + 1):\n",
    "    clustering = cluster.AgglomerativeClustering(\n",
    "        n_clusters=i,\n",
    "    ).fit(data_to_clustering)\n",
    "\n",
    "    ss = metrics.silhouette_score(data_to_clustering, clustering.labels_)\n",
    "    silhouette_agglomerative.append(ss)\n",
    "\n",
    "    dbs = metrics.davies_bouldin_score(data_to_clustering, clustering.labels_)\n",
    "    davies_agglomerative.append(dbs)\n",
    "\n",
    "    chs = metrics.calinski_harabasz_score(data_to_clustering, clustering.labels_)\n",
    "    calinski_agglomerative.append(chs)\n",
    "\n",
    "    agglomerative_labels = clustering.fit_predict(data_to_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Look at clusters by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe for labels\n",
    "agglomerative_labels_labels_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Index\": [_ for _ in range(len(agglomerative_labels))],\n",
    "        \"Label\": list(agglomerative_labels),\n",
    "    },\n",
    "    columns=[\"Index\", \"Label\"],\n",
    ")\n",
    "\n",
    "# Group dataframe by label value\n",
    "label_counts = agglomerative_labels_labels_df.groupby(\"Label\").size()\n",
    "\n",
    "# Show label distribution\n",
    "label_counts.plot.bar()\n",
    "\n",
    "plt.title(\"Clusters from agglomerative\")\n",
    "plt.xlabel(\"Cluster label\")\n",
    "plt.ylabel(\"Cluster entries\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set metric data lists\n",
    "silhouette_KMeans = []\n",
    "davies_KMeans = []\n",
    "calinski_KMeans = []\n",
    "distortions_KMeans = []\n",
    "\n",
    "# Iterate over cluster number\n",
    "for i in range(2, cluster_amount + 1):\n",
    "    clustering = cluster.KMeans(\n",
    "        n_clusters=i,\n",
    "        random_state=0,\n",
    "        n_init=\"auto\",\n",
    "    ).fit(data_to_clustering)\n",
    "\n",
    "    ss = metrics.silhouette_score(data_to_clustering, clustering.labels_)\n",
    "    silhouette_KMeans.append(ss)\n",
    "\n",
    "    dbs = metrics.davies_bouldin_score(data_to_clustering, clustering.labels_)\n",
    "    davies_KMeans.append(dbs)\n",
    "\n",
    "    chs = metrics.calinski_harabasz_score(data_to_clustering, clustering.labels_)\n",
    "    calinski_KMeans.append(chs)\n",
    "\n",
    "    distortions_KMeans.append(clustering.inertia_)\n",
    "\n",
    "    kmeans_labels = clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Look at clusters by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe for labels\n",
    "kmeans_labels_df = pd.DataFrame(\n",
    "    {\"Index\": [_ for _ in range(len(kmeans_labels))], \"Label\": list(kmeans_labels)},\n",
    "    columns=[\"Index\", \"Label\"],\n",
    ")\n",
    "\n",
    "# Group dataframe by label value\n",
    "label_counts = kmeans_labels_df.groupby(\"Label\").size()\n",
    "\n",
    "# Show label distribution\n",
    "label_counts.plot.bar()\n",
    "\n",
    "plt.title(\"Clusters from kmeans\")\n",
    "plt.xlabel(\"Cluster label\")\n",
    "plt.ylabel(\"Cluster entries\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Normalize metrics data for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set function to normalize list\n",
    "def get_list_normalized(list_to_normalize: list) -> list:\n",
    "    max_value = max(list_to_normalize)\n",
    "\n",
    "    return [_ / max_value for _ in list_to_normalize]\n",
    "\n",
    "\n",
    "# Normalize lists with metrics data\n",
    "# For agglomerative\n",
    "silhouette_agglomerative_normalized = get_list_normalized(silhouette_agglomerative)\n",
    "davies_agglgomerative_normalized = get_list_normalized(davies_agglomerative)\n",
    "calinski_agglomerative_normalized = get_list_normalized(calinski_agglomerative)\n",
    "\n",
    "# For k-means\n",
    "silhouette_KMeans_normalized = get_list_normalized(silhouette_KMeans)\n",
    "davies_KMeans_normalized = get_list_normalized(davies_KMeans)\n",
    "calinski_KMeans_normalized = get_list_normalized(calinski_KMeans)\n",
    "distortions_KMeans_normalized = get_list_normalized(distortions_KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set function to plot metrics\n",
    "def get_list_plot(*lists):\n",
    "    for _, data_list in enumerate(lists, 1):\n",
    "        plt.plot(\n",
    "            range(1, len(data_list) + 1),\n",
    "            data_list,\n",
    "            marker=\"o\",\n",
    "        )\n",
    "\n",
    "\n",
    "# Plot metrics for agglomerative\n",
    "get_list_plot(\n",
    "    silhouette_agglomerative_normalized,\n",
    "    davies_agglgomerative_normalized,\n",
    "    calinski_agglomerative_normalized,\n",
    ")\n",
    "\n",
    "plt.title(\"Metrics for agglomerative method\")\n",
    "plt.xlabel(\"Cluster amount\")\n",
    "plt.ylabel(\"Normalized metrics value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Metrics gives unclear view of cluster number but some effect is present between 2 and 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_list_plot(\n",
    "    silhouette_KMeans_normalized,\n",
    "    davies_KMeans_normalized,\n",
    "    calinski_KMeans_normalized,\n",
    "    distortions_KMeans_normalized,\n",
    ")\n",
    "\n",
    "plt.title(\"Metrics for k-means method\")\n",
    "plt.xlabel(\"Cluster amount\")\n",
    "plt.ylabel(\"Normalized metrics value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Metrics gives unclear view of cluster number but some effect is present between 3 and 5 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HDBSCAN clustering\n",
    "clustering = cluster.HDBSCAN(\n",
    "    min_cluster_size=10,\n",
    "    cluster_selection_epsilon=0.5,\n",
    ").fit(data)\n",
    "\n",
    "# Get cluster labels\n",
    "hdbscan_labels = clustering.labels_\n",
    "\n",
    "# Get metrics\n",
    "ss = metrics.silhouette_score(data_to_clustering, hdbscan_labels)\n",
    "dbs = metrics.davies_bouldin_score(data_to_clustering, hdbscan_labels)\n",
    "chs = metrics.calinski_harabasz_score(data_to_clustering, hdbscan_labels)\n",
    "\n",
    "# Get dataframe for labels\n",
    "hdbscan_labels_df = pd.DataFrame(\n",
    "    {\"Index\": [_ for _ in range(len(hdbscan_labels))], \"Label\": list(hdbscan_labels)},\n",
    "    columns=[\"Index\", \"Label\"],\n",
    ")\n",
    "\n",
    "# Filter insufficient labels (-1, -2 etc)\n",
    "hdbscan_labels_filtered = hdbscan_labels_df[hdbscan_labels_df[\"Label\"] > -1]\n",
    "\n",
    "# Group dataframe by label value\n",
    "label_counts = hdbscan_labels_filtered.groupby(\"Label\").size()\n",
    "\n",
    "# Show label distribution\n",
    "label_counts.plot.bar()\n",
    "\n",
    "plt.title(\"Clusters from HDBSCAN\")\n",
    "plt.xlabel(\"Cluster label\")\n",
    "plt.ylabel(\"Cluster entries\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We have 2 clusters which dominate in dataset. But for the margin 3 clusters could be accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe for metrics\n",
    "hdbscan_metrics = pd.DataFrame(\n",
    "    {\n",
    "        \"metrics\": [\"ss\", \"dbs\", \"chs\"],\n",
    "        \"metrics_data\": [ss, dbs, chs],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Show HDBSCAN metrics\n",
    "hdbscan_metrics.plot.bar(\n",
    "    x=\"metrics\",\n",
    "    y=\"metrics_data\",\n",
    "    legend=None,\n",
    ")\n",
    "\n",
    "plt.title(\"Metrics for HDBSCAN\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## CLASTERGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clustergram for maximal cluster amount\n",
    "clustergram = Clustergram(\n",
    "    k_range=range(1, cluster_amount),\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Fit data\n",
    "clustergram.fit(data_to_clustering)\n",
    "\n",
    "# Show clustergram\n",
    "clustergram.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Starting from the number of clusters 3 we observe 3 main threads which are looked stable with not sufficient fluence of entries between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Processing\n",
    "\n",
    "## Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get principal component analyzer\n",
    "pca = decomposition.PCA(random_state=0)\n",
    "\n",
    "# Fit scaled data\n",
    "X_pca = pca.fit(data_to_clustering)\n",
    "\n",
    "# Get explained variance (amount of variance explained by each of the selected components)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Get cumulative explained variance for retained features\n",
    "cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Plot explained variance\n",
    "sns.lineplot(\n",
    "    data=explained_variance_ratio,\n",
    "    label=\"variance\",\n",
    "    color=\"g\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"explained_variance_ratio\")\n",
    "axis_2 = plt.gca().twinx()\n",
    "\n",
    "sns.lineplot(\n",
    "    data=cumulative_explained_variance_ratio,\n",
    "    label=\"cumulative variance\",\n",
    "    color=\"r\",\n",
    "    marker=\"s\",\n",
    ")\n",
    "\n",
    "plt.title(\"Explained variance by principal components\")\n",
    "plt.ylabel(\"cumulative_explained_variance_ratio\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get amount of retained components with cumulative explained variance more than 70%\n",
    "retained_components = np.argmax(cumulative_explained_variance_ratio >= 0.71)\n",
    "\n",
    "print(f\"Number of retained components: {retained_components}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get analyzer for only components which cumulative explained variance is more than 70%\n",
    "pca_reduced = decomposition.PCA(n_components=retained_components)\n",
    "\n",
    "# Get decomposition for chosen components\n",
    "X_reduced = pca_reduced.fit_transform(X_scaled)\n",
    "\n",
    "# Get variance loss\n",
    "loss = 1 - cumulative_explained_variance_ratio[retained_components]\n",
    "\n",
    "print(f\"Variance loss is {loss:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reduced dataset clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HDBSCAN clustering\n",
    "clustering = cluster.HDBSCAN(\n",
    "    min_cluster_size=10,\n",
    "    cluster_selection_epsilon=0.5,\n",
    ").fit(X_reduced)\n",
    "\n",
    "# Get cluster labels\n",
    "hdbscan_labels = clustering.labels_\n",
    "\n",
    "# Get metrics\n",
    "ss = metrics.silhouette_score(data_to_clustering, hdbscan_labels)\n",
    "dbs = metrics.davies_bouldin_score(data_to_clustering, hdbscan_labels)\n",
    "chs = metrics.calinski_harabasz_score(data_to_clustering, hdbscan_labels)\n",
    "\n",
    "# Get dataframe for labels\n",
    "hdbscan_labels_df = pd.DataFrame(\n",
    "    {\"Index\": [_ for _ in range(len(hdbscan_labels))], \"Label\": list(hdbscan_labels)},\n",
    "    columns=[\"Index\", \"Label\"],\n",
    ")\n",
    "\n",
    "# Filter insufficient labels (-1, -2 etc)\n",
    "hdbscan_labels_filtered = hdbscan_labels_df[hdbscan_labels_df[\"Label\"] > -1]\n",
    "\n",
    "# Group dataframe by label value\n",
    "label_counts = hdbscan_labels_filtered.groupby(\"Label\").size()\n",
    "\n",
    "# Show label distribution\n",
    "label_counts.plot.bar()\n",
    "\n",
    "plt.title(\"Clusters from HDBSCAN\")\n",
    "plt.xlabel(\"Cluster label\")\n",
    "plt.ylabel(\"Cluster entries\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We have 2 clusters which dominate in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clustergram for maximal cluster amount\n",
    "clustergram = Clustergram(\n",
    "    k_range=range(1, cluster_amount),\n",
    "    method=\"kmeans\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Fit data\n",
    "clustergram.fit(X_reduced)\n",
    "\n",
    "# Show clustergram\n",
    "clustergram.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It's strange but after reduction clustergram still shows 3 threads("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Summary\n",
    "1. Three methods of clustering used: k-means, hierarchical and HDBSCAN.\n",
    "2. Three dominating clusters are found in non-reduced dataset.\n",
    "3. After decomposition two dominating clusters are found.\n",
    "4. Conservative assessment is three clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
